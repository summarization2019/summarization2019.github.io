**Invited Talks** 

* <a href="https://violetpeng.github.io/">Nanyun Peng</a> (USC ISI) <br>
Title: Creative Generation as Inverse Summarization <br>
Abstract: Recent advances in data-driven approaches have demonstrated appealing results in generating natural languages in applications like machine translation and summarization.  However, when the generation tasks are open-ended and the content is under-specified, existing techniques struggle to demonstrate coherence and creativity in writing. This happens because the generation models are trained to model the surface form (i.e. sequences of words), rather than the more advanced semantics and discourse structures.  Moreover, composing creative pieces such as puns, poems, and stories require deviating from the norm, whereas existing generation approaches seek to mimic the norm and thus are unlikely to lead to truly novel, creative composition.  In this talk, I will talk about several works done by my group and collaborators on creative story and pun generation. We emphasize the importance of understanding the semantics and discourse structures of stories and puns to achieve coherent and creative generation. <br>



* <a href="http://u.cs.biu.ac.il/~dagan/">Ido Dagan</a> (Bar-Ilan University) <br>
Title: Comprehending multi-text information: interaction and consolidation <br>
For almost any topic of interest, information is spread across a
substantial number of texts, whose exploration becomes a hard and
tedious process. Multi Document Summarization aspires to address this
difficulty by providing a concise summary of the targeted information.
However, once reading such (static) summary, an interested user would
typically want additional information, being forced back to the
tedious process of exploring the original texts. In the first part of
the talk I will advocate that this “summarization gap” can be
effectively addressed by an interactive summarization process, where
additional summarized information is presented dynamically in response
to user interactions. Critically, to establish interactive
summarization as a viable research area, we propose a systematic
replicable evaluation framework, based on adapting and extending
evaluation paradigms for traditional static summarization. I will then
present a rather broad framework, and an initial prototype, for
implementing interactive summarization through Query-Focused Summary
Expansion.

The second part of the talk addresses the critical need to consolidate
multi-text information in order to present it most effectively.
Consolidation should be performed at the level of minimal information
units, to properly address issues such as conflating redundancy,
connecting complementary information pieces and detecting
discrepancies across texts, such as contradictions and multiple views.
To realize such consolidation, we propose an information decomposition
framework based on natural-language question-answer pairs, following
and extending the QA-SRL paradigm, accompanied by mechanisms for
linking co-referring statements. We envision that this framework can
provide an effective basis for multi-text summarization, both static
and interactive, as well as for other multi-text tasks. <br>

* <a href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a> (The Hong Kong Polytechnic University) <br>
Title: Beyond Sequence-to-Sequence Neural Summarization <br>


* <a href="http://www.lr.pi.titech.ac.jp/~oku/index-e.html">Manabu Okumura</a> (Tokyo Institute of Technology) <br>
Title: Studying the past in text summarization <br>
Abstract: While the research field of text summarization has a long history of more than 60 years, we have witnessed the remarkable changes and improvements in the summarization researches in these ten years. Integer programming frameworks could improve the performance of summarization models. More recently neural models can be said to enable us to output abstractive summaries, and have changed the research community drastically. However, we should sometimes stop rushing and revisit the past researches. In this talk, we will mention some points in them that might be suggestive for the neural network-based summarization models, especially the length constraint, that can be the most fundamental for the summarization models but has been often ignored. <br>




