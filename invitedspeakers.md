**Invited Talks** 

* <a href="https://violetpeng.github.io/">Nanyun Peng</a> (USC ISI) <br>
Title: TBD <br>


* <a href="http://u.cs.biu.ac.il/~dagan/">Ido Dagan</a> (Bar-Ilan University) <br>
Title: Comprehending multi-text information: interaction and consolidation <br>


* <a href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a> (The Hong Kong Polytechnic University) <br>
Title: TBD <br>


* <a href="http://www.lr.pi.titech.ac.jp/~oku/index-e.html">Manabu Okumura</a> (Tokyo Institute of Technology) <br>
Title: Studying the past in text summarization <br>
Abstract: While the research field of text summarization has a long history of more than 60 years, we have witnessed the remarkable changes and improvements in the summarization researches in these ten years. Integer programming frameworks could improve the performance of summarization models. More recently neural models can be said to enable us to output abstractive summaries, and have changed the research community drastically. However, we should sometimes stop rushing and revisit the past researches. In this talk, we will mention some points in them that might be suggestive for the neural network-based summarization models, especially the length constraint, that can be the most fundamental for the summarization models but has been often ignored. <br>




