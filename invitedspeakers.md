**Invited Talks** 

* <a href="https://violetpeng.github.io/">Nanyun Peng</a> (USC ISI) <br>
*Title*: Creative Generation as Inverse Summarization <br>
*Abstract*: Recent advances in data-driven approaches have demonstrated appealing results in generating natural languages in applications like machine translation and summarization.  However, when the generation tasks are open-ended and the content is under-specified, existing techniques struggle to demonstrate coherence and creativity in writing. This happens because the generation models are trained to model the surface form (i.e. sequences of words), rather than the more advanced semantics and discourse structures.  Moreover, composing creative pieces such as puns, poems, and stories require deviating from the norm, whereas existing generation approaches seek to mimic the norm and thus are unlikely to lead to truly novel, creative composition.  In this talk, I will talk about several works done by my group and collaborators on creative story and pun generation. We emphasize the importance of understanding the semantics and discourse structures of stories and puns to achieve coherent and creative generation. <br>
*Bio*: Nanyun Peng is a Research Assistant Professor at the University of Southern California Computer Science Department, and a Research Lead at the USC Information Sciences Institute. She received a Ph.D. from Johns Hopkins University the Center for Language and Speech Processing. Her research focuses on creative language generation, robustness and generalizability of natural language understanding. She has published more than 30 papers in top NLP/AI conferences such as ACL, EMNLP, AAAI, TACL. Her research has been funded by several DARPA, IARPA, and NIH awards. <br>


* <a href="http://u.cs.biu.ac.il/~dagan/">Ido Dagan</a> (Bar-Ilan University) <br>
*Title*: Comprehending multi-text information: interaction and consolidation <br>
*Abstract*: For almost any topic of interest, information is spread across a
substantial number of texts, whose exploration becomes a hard and
tedious process. Multi Document Summarization aspires to address this
difficulty by providing a concise summary of the targeted information.
However, once reading such (static) summary, an interested user would
typically want additional information, being forced back to the
tedious process of exploring the original texts. In the first part of
the talk I will advocate that this “summarization gap” can be
effectively addressed by an interactive summarization process, where
additional summarized information is presented dynamically in response
to user interactions. Critically, to establish interactive
summarization as a viable research area, we propose a systematic
replicable evaluation framework, based on adapting and extending
evaluation paradigms for traditional static summarization. I will then
present a rather broad framework, and an initial prototype, for
implementing interactive summarization through Query-Focused Summary
Expansion. <br> <br>
The second part of the talk addresses the critical need to consolidate
multi-text information in order to present it most effectively.
Consolidation should be performed at the level of minimal information
units, to properly address issues such as conflating redundancy,
connecting complementary information pieces and detecting
discrepancies across texts, such as contradictions and multiple views.
To realize such consolidation, we propose an information decomposition
framework based on natural-language question-answer pairs, following
and extending the QA-SRL paradigm, accompanied by mechanisms for
linking co-referring statements. We envision that this framework can
provide an effective basis for multi-text summarization, both static
and interactive, as well as for other multi-text tasks. <br>

* <a href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a> (The Hong Kong Polytechnic University) <br>
*Title*: Beyond Sequence-to-Sequence Neural Summarization <br>
*Bio*: Dr. Wenjie Li received the BSc and MSc degrees from Tianjin University,
China, and the PhD degree from the Department of Systems Engineering and
Engineering Management, Chinese University of Hong Kong, Hong Kong.
Currently she is an associate professor in the Department of Computing,
Hong Kong Polytechnic University. Her research interests include text
summarization, machine conversation, natural language understanding and
generation. She has directed and participated in quite a number of
research projects and published more than 200 papers in major
international journals and conference proceedings, including IEEE
Transactions on Knowledge and Data Engineering, IEEE Transactions on
Neural Networks and Learning Systems, ACM Transactions on Information
Systems, Computational Linguistics, ACL, AAAI, IJCAI, WWW, and SIGIR.
She has served as the information officer of SIGHAN and the acting
editor of Transactions of the Association for Computational Linguistics. <br>


* <a href="http://www.lr.pi.titech.ac.jp/~oku/index-e.html">Manabu Okumura</a> (Tokyo Institute of Technology) <br>
*Title*: Studying the past in text summarization <br>
*Abstract*: While the research field of text summarization has a long history of more than 60 years, we have witnessed the remarkable changes and improvements in the summarization researches in these ten years. Integer programming frameworks could improve the performance of summarization models. More recently neural models can be said to enable us to output abstractive summaries, and have changed the research community drastically. However, we should sometimes stop rushing and revisit the past researches. In this talk, we will mention some points in them that might be suggestive for the neural network-based summarization models, especially the length constraint, that can be the most fundamental for the summarization models but has been often ignored. <br>
*Bio*: Manabu Okumura is currently a professor at Institute of Innovative
Research, Tokyo Institute of Technology. He is also a vice-chairperson
of Association for Natural Language Processing (ANLP) in Japan.  He
was a visiting associate professor at the Department of Computer
Science, University of Toronto from 1997 to 1998.  He co-organized a
series of Text Summarization Challenge (TSC), a text summarization
evaluation, the first of its kind in Japan, as a part of the NTCIR
(NII-NACSIS Test Collection for IR Systems) Workshop.  He won Google
AI Focused Research Awards in Japan (2019), Google Research Award
(2015), and IBM Faculty Award (2015).  He plays many key roles in
academic societies, journals and top-tier/representative conferences,
including editor-in-chief for ANLP in Japan (2016-2018), ICICTES
Conference Chair (2015-2019), Chair for JSAI isAI 2011, Area co-chair
for ACL-HLT 2011, Workshop co-chair for PAKDD 2009, Area chair for
EMNLP-CoNLL 2007, Area chair for IJCNLP 2004, Tutorial Co-Chair for
PRICAI 2016, and Program Committee for many conferences (ACL, EACL,
EMNLP, IJCAI, AAAI, WWW, ICCPOL, ICWSM, PRICAI, PAKDD, IJCNLP). <br>



