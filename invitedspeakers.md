**Invited Talks** 

* <a href="https://violetpeng.github.io/">Nanyun Peng</a> (USC ISI) <br>
Title: Creative Generation as Inverse Summarization <br>
Abstract: Recent advances in data-driven approaches have demonstrated appealing results in generating natural languages in applications like machine translation and summarization.  However, when the generation tasks are open-ended and the content is under-specified, existing techniques struggle to demonstrate coherence and creativity in writing. This happens because the generation models are trained to model the surface form (i.e. sequences of words), rather than the more advanced semantics and discourse structures.  Moreover, composing creative pieces such as puns, poems, and stories require deviating from the norm, whereas existing generation approaches seek to mimic the norm and thus are unlikely to lead to truly novel, creative composition.  In this talk, I will talk about several works done by my group and collaborators on creative story and pun generation. We emphasize the importance of understanding the semantics and discourse structures of stories and puns to achieve coherent and creative generation. <br>



* <a href="http://u.cs.biu.ac.il/~dagan/">Ido Dagan</a> (Bar-Ilan University) <br>
Title: Comprehending multi-text information: interaction and consolidation <br>


* <a href="https://www4.comp.polyu.edu.hk/~cswjli/">Wenjie Li</a> (The Hong Kong Polytechnic University) <br>
Title: Beyond Sequence-to-Sequence Neural Summarization <br>


* <a href="http://www.lr.pi.titech.ac.jp/~oku/index-e.html">Manabu Okumura</a> (Tokyo Institute of Technology) <br>
Title: Studying the past in text summarization <br>
Abstract: While the research field of text summarization has a long history of more than 60 years, we have witnessed the remarkable changes and improvements in the summarization researches in these ten years. Integer programming frameworks could improve the performance of summarization models. More recently neural models can be said to enable us to output abstractive summaries, and have changed the research community drastically. However, we should sometimes stop rushing and revisit the past researches. In this talk, we will mention some points in them that might be suggestive for the neural network-based summarization models, especially the length constraint, that can be the most fundamental for the summarization models but has been often ignored. <br>




